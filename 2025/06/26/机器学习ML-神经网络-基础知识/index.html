<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Dr.yuan">





<title>[机器学习ML]神经网络-基础知识 | Ape-tech</title>



    <link rel="icon" href="/blog.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const pagebody = document.getElementsByTagName('body')[0]

            function setTheme(status) {

                if (status === 'dark') {
                    window.sessionStorage.theme = 'dark'
                    pagebody.classList.add('dark-theme');

                } else if (status === 'light') {
                    window.sessionStorage.theme = 'light'
                    pagebody.classList.remove('dark-theme');
                }
            };

            setTheme(window.sessionStorage.theme)
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Ape-tech wiki2025</a></div>
            <div class="menu navbar-right">
                
                <a class="menu-item" href="/archives">Posts</a>
                
                <a class="menu-item" href="/category">Categories</a>
                
                <a class="menu-item" href="/tag">Tags</a>
                
                <a class="menu-item" href="/about">About</a>
                
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Ape-tech wiki2025</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">
                    <svg class="menu-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M4.5 17.27q-.213 0-.356-.145T4 16.768t.144-.356t.356-.143h15q.213 0 .356.144q.144.144.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.144T4 11.999t.144-.356t.356-.143h15q.213 0 .356.144t.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.143Q4 7.443 4 7.23t.144-.356t.356-.143h15q.213 0 .356.144T20 7.23t-.144.356t-.356.144z"/></svg>
                    <svg class="close-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><!-- Icon from Material Symbols Light by Google - https://github.com/google/material-design-icons/blob/master/LICENSE --><path fill="currentColor" d="m12 12.708l-5.246 5.246q-.14.14-.344.15t-.364-.15t-.16-.354t.16-.354L11.292 12L6.046 6.754q-.14-.14-.15-.344t.15-.364t.354-.16t.354.16L12 11.292l5.246-5.246q.14-.14.345-.15q.203-.01.363.15t.16.354t-.16.354L12.708 12l5.246 5.246q.14.14.15.345q.01.203-.15.363t-.354.16t-.354-.16z"/></svg>
                </div>
            </div>
            <div class="menu" id="mobile-menu">
                
                <a class="menu-item" href="/archives">Posts</a>
                
                <a class="menu-item" href="/category">Categories</a>
                
                <a class="menu-item" href="/tag">Tags</a>
                
                <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if (toggleMenu.classList.contains("active")) {
            toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        } else {
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">[机器学习ML]神经网络-基础知识</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Dr.yuan</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">June 26, 2025&nbsp;&nbsp;13:18:48</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="二、基础知识-神经网络"><a href="#二、基础知识-神经网络" class="headerlink" title="二、基础知识-神经网络"></a>二、基础知识-神经网络</h1><h3 id="一、前言-背景"><a href="#一、前言-背景" class="headerlink" title="一、前言&#x2F;背景"></a>一、前言&#x2F;背景</h3><img src="https://i.loli.net/2021/10/31/Pgb94UnW85YKrVN.png" width="20%">

<p>在开始介绍前，有一些知识可以先记在心里：</p>
<p>设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；</p>
<p>神经网络结构图中的拓扑与箭头代表着预测过程时数据的流向，跟训练时的数据流有一定的区别；</p>
<p>结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的权重（其值称为权值），这是需要训练得到的。</p>
<p><strong>历史</strong></p>
<ul>
<li>1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP。在下文中，我们会具体介绍神经元模型。</li>
<li>1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞的突触（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。</li>
<li>1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字–“感知器”（Perceptron）（有的文献翻译成“感知机”，下文统一用“感知器”来指代）。</li>
<li>Minsky1969年出版了一本叫《Perceptron》的书认为，如果将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。</li>
<li>1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。</li>
<li>90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。</li>
<li>2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词–“深度学习”。</li>
<li>深度学习在语音识别领域暂露头角。接着，2012年，深度学习技术又在图像识别领域大展拳脚。Hinton与他的学生在ImageNet竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率15%的好成绩，这个成绩比第二名高了近11个百分点，充分证明了多层神经网络识别效果的优越性。</li>
<li>目前，深度神经网络在人工智能界占据统治地位。但凡有关人工智能的产业报道，必然离不开深度学习。神经网络界当下的四位引领者除了前文所说的Ng，Hinton以外，还有CNN的发明人Yann Lecun，以及《Deep Learning》的作者Bengio。</li>
</ul>
<img src="https://i.loli.net/2021/10/31/aS9rwVhNWeP23Ay.png" width="85%">

<p>人工智能-神经网络发展的曲折兴衰</p>
<h3 id="二、神经元-MP模型"><a href="#二、神经元-MP模型" class="headerlink" title="二、神经元-MP模型"></a>二、神经元-MP模型</h3><p>神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。</p>
<p>下图是一个典型的神经元模型：包含有3个输入，1个输出，以及2个计算功能。</p>
<p>注意中间的箭头线。这些线称为“连接”。每个上有一个“权值”。</p>
<img src="https://i.loli.net/2021/10/31/5EKTx1q7edpcmHX.png" width="40%">

<p>我们使用a来表示输入，用w来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是a，端中间有加权参数w，经过这个加权后的信号会变成a<em>w，因此在连接的末端，信号的大小就变成了a</em>w。</p>
<p>如果我们将神经元图中的所有变量用符号表示，并且写出输出的计算公式的话，就是下图。<br>　<br>　<img src="https://i.loli.net/2021/10/31/dzuCyfKlA9P3BcX.png" width="40%"></p>
<p>可见z是在输入和权值的线性加权和叠加了一个函数g的值。在MP模型里，函数g是sgn函数，也就是取符号函数。这个函数当输入大于0时，输出1，否则输出0。</p>
<p><strong>神经元的扩展</strong></p>
<p>下面对神经元模型的图进行一些扩展。首先将sum函数与sgn函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入a与输出z写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。</p>
<p>神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。</p>
<img src="https://i.loli.net/2021/10/31/NJ5bzUK1kwR769e.png" width="50%">

<p>神经元模型的使用可以这样理解：</p>
<p>我们有一个数据，称之为样本。样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性预测未知属性。具体办法就是使用神经元的公式进行计算。三个已知属性的值是a1，a2，a3，未知属性的值是z。z可以通过公式计算出来。</p>
<p>这里，已知的属性称之为特征，未知的属性称之为目标。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值w1，w2，w3。那么，我们就可以通过神经元模型预测新样本的目标。</p>
<h3 id="三、单层神经网络-感知器"><a href="#三、单层神经网络-感知器" class="headerlink" title="三、单层神经网络-感知器"></a>三、单层神经网络-感知器</h3><p>把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”</p>
<p>在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变，于是我们就有了下图：</p>
<p>在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。</p>
<p>我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。但在本文里，我们根据计算层的数量来命名。</p>
<p>假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。</p>
<img src="https://i.loli.net/2021/10/31/RK7WOMEGrPtusqo.png" width="30%">

<p>们改用二维的下标，用wx,y来表达一个权值。下标中的x代表后一层神经元的序号，而y代表前一层神经元的序号（序号的顺序从上到下）。</p>
<img src="https://i.loli.net/2021/10/31/ePNFHzsho7wYfxR.png" width="40%">

<p>这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。</p>
<p>例如，输入的变量是<code>[a1，a2，a3]T</code>（代表由a1，a2，a3组成的列向量），用向量a来表示。方程的左边是<code>[z1，z2]T</code>，用向量z来表示。</p>
<p>系数则是矩阵W（2行3列的矩阵，排列形式与公式中的一样）。</p>
<p>于是，输出公式可以改写成：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">`g(W * a) = z;`</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个公式就是神经网络中从前一层计算后一层的矩阵运算。</p>
<p>与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个逻辑回归模型，可以做线性分类任务。</p>
<p>我们可以用决策分界来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。</p>
<p><strong>一个实例</strong></p>
<p>城里正在举办一年一度的游戏动漫展览，小明拿不定主意，周末要不要去参观。他决定考虑三个因素。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* 天气：周末是否晴天？       权重为8</span><br><span class="line">* 同伴：能否找到人一起去？   权重为4</span><br><span class="line">* 价格：门票是否可承受？     权重为4</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这时，还需要指定一个阈值（threshold）。如果总和大于阈值，感知器输出1，否则输出0。假定阈值为8，那么 12 &gt; 8，小明决定去参观。阈值的高低代表了意愿的强烈，阈值越低就表示越想去，越高就越不想去。</span><br><span class="line"></span><br><span class="line">`x`表示各种外部因素，`w`表示对应的权重。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">* 外部因素 x1、x2、x3 写成矢量 &lt;x1, x2, x3&gt;，简写为 x</span><br><span class="line">* 权重 w1、w2、w3 也写成矢量 (w1, w2, w3)，简写为 w</span><br><span class="line">* 定义运算 w⋅x = ∑ wx，即 w 和 x 的点运算，等于因素与权重的乘积之和</span><br><span class="line">* 定义 b 等于负的阈值 b = -threshold</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>即有</p>
<img src="https://i.loli.net/2021/10/31/DbWLMYtG7IHuSJN.png" width="40%">

<p><strong>目标：训练权重w， 阈值b。</strong></p>
<h3 id="四、两层神经网络"><a href="#四、两层神经网络" class="headerlink" title="四、两层神经网络"></a>四、两层神经网络</h3><p>与单层神经网络不同，理论上——<strong>两层神经网络可以无限逼近任意连续函数</strong>。</p>
<p>两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。</p>
<p>简化的两层神经网络:</p>
<img src="https://i.loli.net/2021/11/03/FE5wizUIMTKjHmX.png" width="40%">

<p>连接输入层和隐藏层的是W1和b1。由X计算得到H十分简单，就是矩阵运算 <code>H = X * W1 + b1</code></p>
<p>连接隐藏层和输出层的是W2和b2。同样是通过矩阵运算进行的 <code>Y = H * W2 + b2</code></p>
<p>等价表达为：</p>
<img src="https://i.loli.net/2021/11/03/I6AzMH2cxGDTKwo.png" width="40%">

<p><code>g(W(1) * a(1) + b(1)) = a(2);  g(W(2) * a(2) + b(2)) = z;</code></p>
<p><strong>隐藏层-激活函数</strong></p>
<p>两层神经网络可以做非线性分类的关键–隐藏层。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。</p>
<img src="https://i.loli.net/2021/11/03/1bO8ygzvf2LqFuZ.png" width="60%">

<ul>
<li>阶跃函数：当输入小于等于0时，输出0；当输入大于0时，输出1。</li>
<li>Sigmoid：当输入趋近于正无穷&#x2F;负无穷时，输出无限接近于1&#x2F;0。到了两层神经网络时，我们使用的最多的是sigmoid函数。</li>
<li>ReLU：当输入小于0时，输出0；当输入大于0时，输出等于输入。ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。</li>
</ul>
<p>【（激活函数active function &#x3D; 转移函数transfer function）都是叠加非线性函数的说法】</p>
<p><strong>eg:</strong>   激活函数具体是怎么计算的呢？</p>
<p>假如经过公式<code>H=X*W1+b1</code>计算得到的H值为：(1,-2,3,-4,7…)，那么经过阶跃函数激活层后就会变为(1,0,1,0,1…)，经过ReLU激活层之后会变为(1,0,3,0,7…)。</p>
<p>需要注意的是，每个隐藏层计算（矩阵线性运算）之后，都需要加一层激活层，要不然该层线性计算是没有意义的。此时的神经网络变成了如下图所示的形式：</p>
<img src="https://i.loli.net/2021/11/03/6bXExr3FqoHn2Yp.png" width="40%">

<p><strong>【输出的整理-构建损失函数】</strong></p>
<p>为了得到输出结果到各个分类的概率值：</p>
<p>在图4中，输出Y的值可能会是(3,1,0.1,0.5)这样的矩阵，诚然我们可以找到里边的最大值“3”，从而找到对应的分类为I，但是这并不直观。我们想让最终的输出为概率，也就是说可以生成像(90%,5%,2%,3%)这样的结果，这样做不仅可以找到最大概率的分类，而且可以知道各个分类计算的概率值。计算公式如下：</p>
<img src="https://i.loli.net/2021/11/03/VF4lbZDEwvqStxs.png" width="20%">

<p>简单来说分三步进行：（1）以e为底对所有元素求指数幂；（2）将所有指数幂求和；（3）分别将这些指数幂与该和做商。</p>
<p>这样求出的结果中，所有元素的和一定为1，而每个元素可以代表概率值。</p>
<p>我们将使用这个计算公式做输出结果正规化处理的层叫做“Softmax”层。</p>
<p><strong>损失函数</strong></p>
<p>机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。首先给所有参数赋上随机值。与目标值构建损失函数，进一步优化<strong>参数w、b</strong>，使训练后的损失函数不断趋近于0。</p>
<ul>
<li>损失函数1：<strong>交叉熵损失（Cross Entropy Error）</strong> ————适合分类问题的概率求解<br>一种直观的解决方法，是用1减去Softmax输出的概率，比如<code>1-90%=0.1</code>。不过更为常用且巧妙的方法是，求对数的负数。<br>还是用90%举例，对数的负数就是：<code>log0.9=0.046</code><br>可以想见，概率越接近100%，该计算结果值越接近于0，说明结果越准确，该输出叫做“交叉熵损失（Cross Entropy Error）”。<br>此时两层神经网络结构如下图：</li>
</ul>
<img src="https://i.loli.net/2021/11/03/J3BdRamyLpuOvHI.png" width="40%">

<ul>
<li>损失函数2：<strong>损失值</strong><br>样本的预测目标为yp，真实目标为y。那么，定义一个值loss，计算公式如下。<code>loss = (yp - y)2</code></li>
</ul>
<p>这个值称之为损失（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。<br>如果将先前的神经网络预测的矩阵公式带入到yp中（因为有z&#x3D;yp），那么我们可以把损失写为关于参数（parameter）的函数，这个函数称之为损失函数。</p>
<p><em><strong>以上内容讲述了神经网络的正向传播过程：神经网络的传播都是形如Y&#x3D;WX+b的矩阵运算；为了给矩阵运算加入非线性，需要在隐藏层中加入激活层；输出层结果需要经过Softmax层处理为概率值，并通过交叉熵损失来量化当前网络的优劣。</strong></em></p>
<p><strong>【反向传播-梯度下降法】</strong></p>
<p>反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播。前缀E代表着相对导数的意思。</p>
<p>反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。</p>
<img src="https://i.loli.net/2021/11/03/Q8kTgzvPAj9YcWu.png" width="40%">

<p><strong>梯度下降</strong></p>
<p>其实反向传播就是一个参数优化的过程，优化对象就是网络中的所有W和b（因为其他所有参数都是确定的）。</p>
<p>神经网络的神奇之处，就在于它可以自动做W和b的优化，在深度学习中，参数的数量有时会上亿，不过其优化的原理和我们这个两层神经网络是一样的。</p>
<p>这里举一个形象的例子描述一下这个参数优化的原理和过程：</p>
<ul>
<li>假设我们操纵着一个球型机器行走在沙漠中，我们在机器中操纵着四个旋钮，分别叫做W1，b1，W2，b2。当我们旋转其中的某个旋钮时，球形机器会发生移动，但是旋转旋钮大小和机器运动方向之间的对应关系是不知道的。而我们的目的就是走到<strong>沙漠的最低点</strong>。</li>
<li>此时我们该怎么办？只能挨个试喽。<br>如果增大W1后，球向上走了，那就减小W1。<br>如果增大b1后，球向下走了，那就继续增大b1。<br>如果增大W2后，球向下走了一大截，那就多增大些W2。<br>。。。</li>
</ul>
<p>这就是进行参数优化的形象解释（有没有想到求导？），这个方法叫做梯度下降法。<br>当我们的球形机器走到最低点时，也就代表着我们的交叉熵损失达到最小（接近于0）。</p>
<p>神经网络需要反复<strong>迭代</strong>。<br>如上述例子中，第一次计算得到的概率是90%，交叉熵损失值是0.046；将该损失值反向传播，使W1,b1,W2,b2做相应微调；再做第二次运算，此时的概率可能就会提高到92%，相应地，损失值也会下降，然后再反向传播损失值，微调参数W1,b1,W2,b2。依次类推，损失值越来越小，直到我们满意为止。</p>
<p>此时我们就得到了理想的W1,b1,W2,b2。</p>
<p>此时如果将任意一组坐标作为输入，利用图4或图5的流程，就能得到分类结果。</p>
<p><strong>学习率</strong></p>
<p>学习率指：在反向传播中，每次传播需达成的损失的优化率的目标。一般取初始值0.05。</p>
<h3 id="五、深度神经网络"><a href="#五、深度神经网络" class="headerlink" title="五、深度神经网络"></a>五、深度神经网络</h3><p>增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。</p>
<p>更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。</p>
<p>更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的容量（capcity）去拟合真正的关系。</p>
<p><strong>网格搜索</strong></p>
<p>在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。</p>
<img src="https://i.loli.net/2021/11/03/4QiHG7Wc8wPSarY.png" width="80%">

<p>上图的网络中，虽然参数数量仍然是33，但却有4个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。</p>
<p><em><strong>节点越多，层次越深————则表达能力更强</strong></em></p>
<p>深度模型：</p>
<p>LSTM长短时记忆与RNN循环神经网络；</p>
<p>Bolzmann机；</p>
<p>CNN卷积神经网络；</p>
<p>GAN生成对抗网络；</p>
<h3 id="六、遗传算法-神经网络-做进化生成"><a href="#六、遗传算法-神经网络-做进化生成" class="headerlink" title="六、遗传算法+神经网络&#x3D;做进化生成"></a>六、遗传算法+神经网络&#x3D;做进化生成</h3><p>神经网络作为GA的适应度评价函数</p>
<p>吴恩达【机器学习】</p>
<p>Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”</p>
<p>Example: playing checkers.</p>
<p>E &#x3D; the experience of playing many games of checkers</p>
<p>T &#x3D; the task of playing checkers.</p>
<p>P &#x3D; the probability that the program will win the next game.</p>
<ul>
<li>[supervised learning]</li>
</ul>
<p>given right answear</p>
<p>Supervised learning problems are categorized into “regression” and “classification” problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.</p>
<p>1、回归问题</p>
<img src="https://s2.loli.net/2022/01/07/cGwjo93F4taCU1h.png" width="80%">

<p>2、分类问题</p>
<img src="https://s2.loli.net/2022/01/07/uWl1VwXPec3vFBS.png" width="80%">

<p>-[unsupervised learning]</p>
<p>Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables.<br>We can derive this structure by clustering the data based on relationships among the variables in the data.</p>
<p>1、分类</p>
<p>在无数据标定的情况下分类数据；</p>
<p>2、非分类</p>
<p>“鸡尾酒会算法”—— ——区分并提取两种重叠声音</p>
<img src="https://s2.loli.net/2022/01/10/Bnjv2FpRG8zSHAt.png" width="80%">

<p>others: Reinforcement learning, recommender systems.</p>
<p>[符号定义]</p>
<img src="https://s2.loli.net/2022/01/10/1jQ9PSohM25yAvg.png" width="60%">

<ul>
<li>m&#x3D;训练集的数据行数</li>
</ul>
<p>-x&#x3D;输入的特征</p>
<p>-y&#x3D;输出的特征（目标值）</p>
<p>（x, y) 一组训练数据</p>
<p>【一个线性回归的示例：】</p>
<img src="https://s2.loli.net/2022/01/10/F3NszZvD4PqojXY.png" width="80%">

<p>【cost function】平方误差函数：</p>
<p>回归的过程：通过调整θ0、θ1参数，求 “平方误差函数” 的最小值；</p>
<img src="https://s2.loli.net/2022/01/10/LHhCsB1nVA9fcgj.png" width="80%">

<p>即取θ1参数，使J(θ1)在曲线取最小值：</p>
<img src="https://s2.loli.net/2022/01/10/eXh2tcSOsARpbkl.png" width="80%">
        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Dr.yuan</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://example.com/2025/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0ML-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">http://example.com/2025/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0ML-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0ML/"># 机器学习ML</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2025/06/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DL-CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">[深度学习DL]CNN卷积神经网络</a>
            
            
            <a class="next" rel="next" href="/2025/06/26/%E6%96%B9%E6%B3%95-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%A5%96%E5%8A%B1-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96/">[方法]马尔可夫-马尔可夫奖励-马尔可夫决策</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Dr.yuan | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>