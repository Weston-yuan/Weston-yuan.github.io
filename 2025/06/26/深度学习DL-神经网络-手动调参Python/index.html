<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Dr.yuan">





<title>[深度学习DL]神经网络-手动调参Python | Ape-tech</title>



    <link rel="icon" href="/blog.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const pagebody = document.getElementsByTagName('body')[0]

            function setTheme(status) {

                if (status === 'dark') {
                    window.sessionStorage.theme = 'dark'
                    pagebody.classList.add('dark-theme');

                } else if (status === 'light') {
                    window.sessionStorage.theme = 'light'
                    pagebody.classList.remove('dark-theme');
                }
            };

            setTheme(window.sessionStorage.theme)
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Ape-tech wiki2025</a></div>
            <div class="menu navbar-right">
                
                <a class="menu-item" href="/archives">Posts</a>
                
                <a class="menu-item" href="/category">Categories</a>
                
                <a class="menu-item" href="/tag">Tags</a>
                
                <a class="menu-item" href="/about">About</a>
                
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Ape-tech wiki2025</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">
                    <svg class="menu-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M4.5 17.27q-.213 0-.356-.145T4 16.768t.144-.356t.356-.143h15q.213 0 .356.144q.144.144.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.144T4 11.999t.144-.356t.356-.143h15q.213 0 .356.144t.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.143Q4 7.443 4 7.23t.144-.356t.356-.143h15q.213 0 .356.144T20 7.23t-.144.356t-.356.144z"/></svg>
                    <svg class="close-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><!-- Icon from Material Symbols Light by Google - https://github.com/google/material-design-icons/blob/master/LICENSE --><path fill="currentColor" d="m12 12.708l-5.246 5.246q-.14.14-.344.15t-.364-.15t-.16-.354t.16-.354L11.292 12L6.046 6.754q-.14-.14-.15-.344t.15-.364t.354-.16t.354.16L12 11.292l5.246-5.246q.14-.14.345-.15q.203-.01.363.15t.16.354t-.16.354L12.708 12l5.246 5.246q.14.14.15.345q.01.203-.15.363t-.354.16t-.354-.16z"/></svg>
                </div>
            </div>
            <div class="menu" id="mobile-menu">
                
                <a class="menu-item" href="/archives">Posts</a>
                
                <a class="menu-item" href="/category">Categories</a>
                
                <a class="menu-item" href="/tag">Tags</a>
                
                <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if (toggleMenu.classList.contains("active")) {
            toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        } else {
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">[深度学习DL]神经网络-手动调参Python</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Dr.yuan</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">June 26, 2025&nbsp;&nbsp;13:30:35</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="一、深度神经网络调参技巧"><a href="#一、深度神经网络调参技巧" class="headerlink" title="一、深度神经网络调参技巧"></a>一、深度神经网络调参技巧</h1><h2 id="1、构建一个DL神经网络"><a href="#1、构建一个DL神经网络" class="headerlink" title="1、构建一个DL神经网络"></a>1、构建一个DL神经网络</h2><p>code something！！！</p>
<h2 id="2、激活函数调整"><a href="#2、激活函数调整" class="headerlink" title="2、激活函数调整"></a>2、激活函数调整</h2><img src="https://s2.loli.net/2023/05/08/xj8e3mgnTfzKD5r.png" width="80%">

<h2 id="3、数据预处理（中心化）"><a href="#3、数据预处理（中心化）" class="headerlink" title="3、数据预处理（中心化）"></a>3、数据预处理（中心化）</h2><p>data在原点处的扰动更小</p>
<img src="https://s2.loli.net/2023/05/08/Qy6quSaLtU5ok98.png" width="60%">

<h2 id="4、weight随机初始化"><a href="#4、weight随机初始化" class="headerlink" title="4、weight随机初始化"></a>4、weight随机初始化</h2><img src="https://s2.loli.net/2023/05/08/qitSfcvZ5s24RAC.png" width="60%">

<h2 id="5、batch-normalization"><a href="#5、batch-normalization" class="headerlink" title="5、batch normalization"></a>5、batch normalization</h2><p>(使data 独立同分布，CNN常用)</p>
<p>可以取代：特征缩放、正则化、减少过拟合</p>
<img src="https://s2.loli.net/2023/05/08/WJxSN9cgs2aLjrD.png" width="80%">

<h3 id="1-在layers-py实现对数据的批量归一化："><a href="#1-在layers-py实现对数据的批量归一化：" class="headerlink" title="(1)在layers.py实现对数据的批量归一化："></a>(1)在layers.py实现对数据的批量归一化：</h3><blockquote>
<p>batchnorm_forward，批量归一前向传播</p>
<p>batchnorm_backward，批量归一反向传播</p>
<p>batchnorm_backward_alt，简化的批量归一化向后传递</p>
</blockquote>
<p>在权重初始化不好的情况下，batchnorm的性能明显优于没有baseline；权重初始化较好的情况下，表现相当。</p>
<p>batch越大对于平均值和方差的估计越准确，结果越好。所以在硬件支持的情况下，batch越大越好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in layers.py.</span></span><br><span class="line"><span class="comment">#————————————1-batchnorm_forward，批量归一前向传播 ——————————————————————</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batchnorm_forward</span>(<span class="params">x, gamma, beta, bn_param</span>):</span><br><span class="line">    mode = bn_param[<span class="string">&#x27;mode&#x27;</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">&#x27;eps&#x27;</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">&#x27;momentum&#x27;</span>, <span class="number">0.9</span>)</span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">&#x27;running_mean&#x27;</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">&#x27;running_var&#x27;</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">        sample_mean = np.mean(x,axis = <span class="number">0</span>)   <span class="comment">#矩阵x每一列的平均值(D,)</span></span><br><span class="line">        sample_var = np.var(x,axis = <span class="number">0</span>)     <span class="comment">#矩阵x每一列的方差(D,)</span></span><br><span class="line">        x_hat = (x - sample_mean)/(np.sqrt(sample_var + eps))   <span class="comment">#标准化，eps:防止除数为0而增加的一个很小的正数</span></span><br><span class="line">        out = gamma * x_hat + beta <span class="comment">#gamma放缩系数，beta偏移常量</span></span><br><span class="line">        cache = (x,sample_mean,sample_var,x_hat,eps,gamma,beta)   <span class="comment"># ！！向后传递所需的任何中间体都应存储在缓存变量cache中。</span></span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean   <span class="comment">#基于动量的参数衰减（论文里的？）</span></span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var  <span class="comment">#基于动量的参数衰减（论文里的？）</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        out = (x - running_mean) * gamma / (np.sqrt(running_var + eps)) + beta <span class="comment"># normalize缩放输入数据X，并存入out （根据论文公式）！！！</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Invalid forward batchnorm mode &quot;%s&quot;&#x27;</span> % mode)</span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">&#x27;running_mean&#x27;</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">&#x27;running_var&#x27;</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"><span class="comment">#————————————2-batchnorm_backward，批量归一反向传播 ——————————————————————</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batchnorm_backward</span>(<span class="params">dout, cache</span>):</span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment"># ！！！利用公式、计算图计算梯度</span></span><br><span class="line">    x,mean,var,x_hat,eps,gamma,beta = cache</span><br><span class="line">    N = x.shape[<span class="number">0</span>]</span><br><span class="line">    dgamma = np.<span class="built_in">sum</span>(dout * x_hat,axis = <span class="number">0</span>)</span><br><span class="line">    dbeta = np.<span class="built_in">sum</span>(dout * <span class="number">1.0</span>,axis = <span class="number">0</span>)</span><br><span class="line">    dx_hat = dout * gamma</span><br><span class="line">    dx_hat_numerator = dx_hat / np.sqrt(var + eps)</span><br><span class="line">    dx_hat_denominator = np.<span class="built_in">sum</span>(dx_hat * (x - mean),axis = <span class="number">0</span>)</span><br><span class="line">    dx_1 = dx_hat_numerator</span><br><span class="line">    dvar = -<span class="number">0.5</span> * ((var + eps) ** (-<span class="number">1.5</span>)) * dx_hat_denominator</span><br><span class="line">    dmean = -<span class="number">1.0</span> * np.<span class="built_in">sum</span>(dx_hat_numerator,axis = <span class="number">0</span>) + dvar * np.mean(-<span class="number">2.0</span> * (x - mean),axis = <span class="number">0</span>)</span><br><span class="line">    dx_var = dvar * <span class="number">2.0</span> / N * (x - mean)</span><br><span class="line">    dx_mean = dmean * <span class="number">1.0</span> / N</span><br><span class="line">    dx = dx_1 + dx_var + dx_mean</span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br><span class="line"></span><br><span class="line"><span class="comment">#————————————3-batchnorm_backward_alt，简化的批量归一化向后传递 ——————————————————————</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batchnorm_backward_alt</span>(<span class="params">dout, cache</span>):</span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">#使用论文中推导的公式计算</span></span><br><span class="line">    x,mean,var,x_hat,eps,gamma,beta = cache</span><br><span class="line">    N = x.shape[<span class="number">0</span>]</span><br><span class="line">    dbeta = np.<span class="built_in">sum</span>(dout,axis = <span class="number">0</span>)</span><br><span class="line">    dgamma = np.<span class="built_in">sum</span>(x_hat * dout,axis = <span class="number">0</span>)</span><br><span class="line">    dx_norm = dout * gamma</span><br><span class="line">    dv = ((x - mean) * -<span class="number">0.5</span> * (var + eps)**-<span class="number">1.5</span> * dx_norm).<span class="built_in">sum</span>(axis = <span class="number">0</span>)</span><br><span class="line">    dm = (dx_norm * -<span class="number">1</span> * (var + eps)** -<span class="number">0.5</span>).<span class="built_in">sum</span>(axis = <span class="number">0</span>) + (dv * (x - mean) * -<span class="number">2</span> / N).<span class="built_in">sum</span>(axis = <span class="number">0</span>)</span><br><span class="line">    dx = dx_norm / (var + eps) ** <span class="number">0.5</span> + dv * <span class="number">2</span> * (x - mean) / N + dm / N</span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="2-在layers-py实现对层的归一化Layer-Normalization："><a href="#2-在layers-py实现对层的归一化Layer-Normalization：" class="headerlink" title="(2)在layers.py实现对层的归一化Layer Normalization："></a>(2)在layers.py实现对层的归一化Layer Normalization：</h3><blockquote>
<p>layernorm_forward，层归一化前向传播</p>
<p>layernorm_backward，层归一化后向传播</p>
</blockquote>
<p>LN是针对深度网络的某一层的所有神经元的输入进行normalize操作。LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差。</p>
<p>LN用于RNN效果比较明显，但是在CNN上，不如BN。</p>
<p>layernorm和batch size的尺寸大小之间没有关系关系。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in layers.py.</span></span><br><span class="line"><span class="comment">#————————————1-layernorm_forward，层归一化前向传播 ——————————————————————</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">layernorm_forward</span>(<span class="params">x, gamma, beta, ln_param</span>):</span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    eps = ln_param.get(<span class="string">&#x27;eps&#x27;</span>, <span class="number">1e-5</span>)</span><br><span class="line">    <span class="comment">#layer norm是对输入的数据的每一个样本（1，D），求均值方差等等，不依赖batch。相比batch norm有自己的特点。但仿佛在卷积网络中效果不如batch norm</span></span><br><span class="line">    x_T = x.T</span><br><span class="line">    sample_mean = np.mean(x_T,axis = <span class="number">0</span>)</span><br><span class="line">    sample_var = np.var(x_T,axis = <span class="number">0</span>)</span><br><span class="line">    x_norm_T = (x_T - sample_mean) / np.sqrt(sample_var + eps)</span><br><span class="line">    x_norm = x_norm_T.T</span><br><span class="line">    out = x_norm * gamma + beta</span><br><span class="line">    cache = (x,x_norm,gamma,sample_mean,sample_var,eps)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"><span class="comment">#————————————2-layernorm_backward，层归一化后向传播 ——————————————————————</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">layernorm_backward</span>(<span class="params">dout, cache</span>):</span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    x,x_norm,gamma,sample_mean,sample_var,eps = cache</span><br><span class="line">    x_T = x.T</span><br><span class="line">    dout_T = dout.T</span><br><span class="line">    N = x_T.shape[<span class="number">0</span>]</span><br><span class="line">    dbeta = np.<span class="built_in">sum</span>(dout,axis = <span class="number">0</span>)</span><br><span class="line">    dgamma = np.<span class="built_in">sum</span>(x_norm * dout,axis = <span class="number">0</span>)</span><br><span class="line">    dx_norm = dout_T * gamma[:,np.newaxis]</span><br><span class="line">    dv = ((x_T - sample_mean) * -<span class="number">0.5</span> * (sample_var + eps)** -<span class="number">1.5</span> * dx_norm).<span class="built_in">sum</span>(axis = <span class="number">0</span>)</span><br><span class="line">    dm = (dx_norm * -<span class="number">1</span> * (sample_var + eps) ** -<span class="number">0.5</span>).<span class="built_in">sum</span>(axis = <span class="number">0</span>) + (dv * (x_T - sample_mean) * -<span class="number">2</span> / N).<span class="built_in">sum</span>(axis = <span class="number">0</span>)</span><br><span class="line">    dx_T = dx_norm / (sample_var + eps)** <span class="number">0.5</span> + dv * <span class="number">2</span> * (x_T - sample_mean) / N + dm / N</span><br><span class="line">    dx = dx_T.T</span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="6、babysitting-学习曲线"><a href="#6、babysitting-学习曲线" class="headerlink" title="6、babysitting 学习曲线"></a>6、babysitting 学习曲线</h2><img src="https://s2.loli.net/2023/05/08/5xkFihuCRN4bOTD.png" width="60%">

<h2 id="7、超参数优化"><a href="#7、超参数优化" class="headerlink" title="7、超参数优化"></a>7、超参数优化</h2><p>（1）网络结构、学习率、正则化值、监控学习曲线</p>
<p>（2）根据ML偏差、方差规则调整</p>
<img src="https://s2.loli.net/2023/05/08/qGLwRxDlvVCPQgf.png" width="60%">

<h2 id="8、高级优化方法Momentum-RMSProp-Adam"><a href="#8、高级优化方法Momentum-RMSProp-Adam" class="headerlink" title="8、高级优化方法Momentum, RMSProp, Adam"></a>8、高级优化方法Momentum, RMSProp, Adam</h2><p>（1）SDG、SDG Momentum</p>
<img src="https://s2.loli.net/2023/05/08/Bc7YRFxkhaDJt5f.png" width="60%">

<p>（2）Nesterov Momentum</p>
<img src="https://s2.loli.net/2023/05/08/9VPdzMJ3XwZrngv.png" width="80%">

<p>（3）Adam</p>
<img src="https://s2.loli.net/2023/05/08/1v7Rr8lYJ9wbB2X.png" width="80%">

<h2 id="9、Regularization改善模型性能"><a href="#9、Regularization改善模型性能" class="headerlink" title="9、Regularization改善模型性能"></a>9、Regularization改善模型性能</h2><h3 id="（1）regularization"><a href="#（1）regularization" class="headerlink" title="（1）regularization"></a>（1）regularization</h3><img src="https://s2.loli.net/2023/05/08/lVZ8ksdvGgEBrRy.png" width="50%">

<h3 id="（2）Dropout"><a href="#（2）Dropout" class="headerlink" title="（2）Dropout"></a>（2）Dropout</h3><p>Dropout是一种正则化神经网络的技术，它通过在前向传递过程中将一些特征随机设置为零</p>
<p>在每一次前向传播中，随机设置一些神经元为零的丢弃概率是一个超参数(例如p&#x3D; 0.5)</p>
<img src="https://s2.loli.net/2023/05/08/qMdLt93uXnZ4OB7.png" width="80%">

<h3 id="在layers-py实现dropout"><a href="#在layers-py实现dropout" class="headerlink" title="在layers.py实现dropout"></a>在layers.py实现dropout</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in layers.py</span></span><br><span class="line"><span class="comment">#————————————1-dropout_forward，dropout前向传播 ——————————————————————</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_forward</span>(<span class="params">x, dropout_param</span>):</span><br><span class="line">    p, mode = dropout_param[<span class="string">&quot;p&quot;</span>], dropout_param[<span class="string">&quot;mode&quot;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;seed&quot;</span> <span class="keyword">in</span> dropout_param:</span><br><span class="line">        np.random.seed(dropout_param[<span class="string">&quot;seed&quot;</span>])</span><br><span class="line">    mask = <span class="literal">None</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">        mask = ( np.random.rand(*x.shape) &lt; p ) / p     <span class="comment">#生成以xshape为大小的0到1的平均分布  这里/p是为了测试和训练保持数学期望</span></span><br><span class="line">        out = x * mask</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">&quot;test&quot;</span>:</span><br><span class="line">        out = x</span><br><span class="line">    cache = (dropout_param, mask)</span><br><span class="line">    out = out.astype(x.dtype, copy=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"><span class="comment">#————————————2-dropout_backward，dropout反向传播 ——————————————————————</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_backward</span>(<span class="params">dout, cache</span>):</span><br><span class="line">    dropout_param, mask = cache      <span class="comment"># 从前向传播来的缓存 (dropout_param, mask)</span></span><br><span class="line">    mode = dropout_param[<span class="string">&quot;mode&quot;</span>]    <span class="comment"># 训练模式和验证模式</span></span><br><span class="line">    dx = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">        dx = dout * mask</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">&quot;test&quot;</span>:</span><br><span class="line">        dx = dout</span><br><span class="line">    <span class="keyword">return</span> dx</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="（3）Data-Augmentation"><a href="#（3）Data-Augmentation" class="headerlink" title="（3）Data Augmentation"></a>（3）Data Augmentation</h3><p>翻转、旋转、伸展、剪切、镜头扭曲、颜色抖动</p>
<img src="https://s2.loli.net/2023/05/08/smCWtS1iorqRbDc.png" width="80%">

<h1 id="二、深度神经网络DL-示例代码code"><a href="#二、深度神经网络DL-示例代码code" class="headerlink" title="二、深度神经网络DL-示例代码code"></a>二、深度神经网络DL-示例代码code</h1><h2 id="1、构建深度（任意层数）神经网络"><a href="#1、构建深度（任意层数）神经网络" class="headerlink" title="1、构建深度（任意层数）神经网络"></a>1、构建深度（任意层数）神经网络</h2><h3 id="（1）主程序FullyConnectedNets-ipynb"><a href="#（1）主程序FullyConnectedNets-ipynb" class="headerlink" title="（1）主程序FullyConnectedNets.ipynb"></a>（1）主程序FullyConnectedNets.ipynb</h3><p>1-初始化程序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ——————————初始化——————————</span></span><br><span class="line"><span class="comment"># Setup cell.</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> cs231n.classifiers.fc_net <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> cs231n.data_utils <span class="keyword">import</span> get_CIFAR10_data</span><br><span class="line"><span class="keyword">from</span> cs231n.gradient_check <span class="keyword">import</span> eval_numerical_gradient, eval_numerical_gradient_array</span><br><span class="line"><span class="keyword">from</span> cs231n.solver <span class="keyword">import</span> Solver</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">&quot;figure.figsize&quot;</span>] = (<span class="number">10.0</span>, <span class="number">8.0</span>)  <span class="comment"># Set default size of plots.</span></span><br><span class="line">plt.rcParams[<span class="string">&quot;image.interpolation&quot;</span>] = <span class="string">&quot;nearest&quot;</span></span><br><span class="line">plt.rcParams[<span class="string">&quot;image.cmap&quot;</span>] = <span class="string">&quot;gray&quot;</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rel_error</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns relative error.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(x - y) / (np.maximum(<span class="number">1e-8</span>, np.<span class="built_in">abs</span>(x) + np.<span class="built_in">abs</span>(y))))</span><br><span class="line"></span><br><span class="line"><span class="comment">#————————加载数据集——————————</span></span><br><span class="line"><span class="comment"># Load the (preprocessed) CIFAR-10 data. 载入图像数据集</span></span><br><span class="line">data = get_CIFAR10_data()</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">list</span>(data.items()):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;k&#125;</span>: <span class="subst">&#123;v.shape&#125;</span>&quot;</span>)  <span class="comment"># 打印数据集尺寸</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>2-初始化loss和梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ——————————初始化loss和梯度————————————————————</span></span><br><span class="line">np.random.seed(<span class="number">231</span>)</span><br><span class="line">N, D, H1, H2, C = <span class="number">2</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">10</span></span><br><span class="line">X = np.random.randn(N, D)</span><br><span class="line">y = np.random.randint(C, size=(N,))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> reg <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">3.14</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Running check with reg = &quot;</span>, reg)</span><br><span class="line">    model = FullyConnectedNet(</span><br><span class="line">        [H1, H2],</span><br><span class="line">        input_dim=D,</span><br><span class="line">        num_classes=C,</span><br><span class="line">        reg=reg,</span><br><span class="line">        weight_scale=<span class="number">5e-2</span>,</span><br><span class="line">        dtype=np.float64</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    loss, grads = model.loss(X, y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Initial loss: &quot;</span>, loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> <span class="built_in">sorted</span>(grads):</span><br><span class="line">        f = <span class="keyword">lambda</span> _: model.loss(X, y)[<span class="number">0</span>]</span><br><span class="line">        grad_num = eval_numerical_gradient(f, model.params[name], verbose=<span class="literal">False</span>, h=<span class="number">1e-5</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> relative error: <span class="subst">&#123;rel_error(grad_num, grads[name])&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3-构建五层神经网络，100个units</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#———————————————五层神经网络—————————————————————</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Use a five-layer Net to overfit 50 training examples by</span></span><br><span class="line"><span class="comment"># tweaking just the learning rate and initialization scale.</span></span><br><span class="line"></span><br><span class="line">num_train = <span class="number">50</span>                                   <span class="comment"># 训练集共50张图像</span></span><br><span class="line">small_data = &#123;                                  <span class="comment"># 划分训练集和验证集</span></span><br><span class="line">  <span class="string">&#x27;X_train&#x27;</span>: data[<span class="string">&#x27;X_train&#x27;</span>][:num_train],</span><br><span class="line">  <span class="string">&#x27;y_train&#x27;</span>: data[<span class="string">&#x27;y_train&#x27;</span>][:num_train],</span><br><span class="line">  <span class="string">&#x27;X_val&#x27;</span>: data[<span class="string">&#x27;X_val&#x27;</span>],</span><br><span class="line">  <span class="string">&#x27;y_val&#x27;</span>: data[<span class="string">&#x27;y_val&#x27;</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-3</span>  <span class="comment"># Experiment with this! 超参数</span></span><br><span class="line">weight_scale = <span class="number">1e-1</span>   <span class="comment"># Experiment with this! 超参数</span></span><br><span class="line">model = FullyConnectedNet(            <span class="comment"># 调用FullyConnectedNet全连接层，设置：五层神经网络，隐藏层100x100x100x100个units！！！！</span></span><br><span class="line">    [<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>],</span><br><span class="line">    weight_scale=weight_scale,</span><br><span class="line">    dtype=np.float64</span><br><span class="line">)</span><br><span class="line">solver = Solver(</span><br><span class="line">    model,</span><br><span class="line">    small_data,</span><br><span class="line">    print_every=<span class="number">10</span>,</span><br><span class="line">    num_epochs=<span class="number">20</span>,</span><br><span class="line">    batch_size=<span class="number">25</span>,</span><br><span class="line">    update_rule=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    optim_config=&#123;<span class="string">&#x27;learning_rate&#x27;</span>: learning_rate&#125;,</span><br><span class="line">)</span><br><span class="line">solver.train()</span><br><span class="line"></span><br><span class="line">plt.plot(solver.loss_history)</span><br><span class="line">plt.title(<span class="string">&#x27;Training loss history&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Training loss&#x27;</span>)</span><br><span class="line">plt.grid(linestyle=<span class="string">&#x27;--&#x27;</span>, linewidth=<span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>4-调用优化器</p>
<p><a target="_blank" rel="noopener" href="http://调用优化器optim.py/">调用优化器optim.py</a> ：在函数 sgd_momentum 中实现 SGD+momentum 更新规则</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cs231n.optim <span class="keyword">import</span> sgd_momentum</span><br><span class="line"></span><br><span class="line">N, D = <span class="number">4</span>, <span class="number">5</span></span><br><span class="line">w = np.linspace(-<span class="number">0.4</span>, <span class="number">0.6</span>, num=N*D).reshape(N, D)</span><br><span class="line">dw = np.linspace(-<span class="number">0.6</span>, <span class="number">0.4</span>, num=N*D).reshape(N, D)</span><br><span class="line">v = np.linspace(<span class="number">0.6</span>, <span class="number">0.9</span>, num=N*D).reshape(N, D)</span><br><span class="line"></span><br><span class="line">config = &#123;<span class="string">&quot;learning_rate&quot;</span>: <span class="number">1e-3</span>, <span class="string">&quot;velocity&quot;</span>: v&#125;</span><br><span class="line">next_w, _ = sgd_momentum(w, dw, config=config)</span><br><span class="line"></span><br><span class="line">expected_next_w = np.asarray([</span><br><span class="line">  [ <span class="number">0.1406</span>,      <span class="number">0.20738947</span>,  <span class="number">0.27417895</span>,  <span class="number">0.34096842</span>,  <span class="number">0.40775789</span>],</span><br><span class="line">  [ <span class="number">0.47454737</span>,  <span class="number">0.54133684</span>,  <span class="number">0.60812632</span>,  <span class="number">0.67491579</span>,  <span class="number">0.74170526</span>],</span><br><span class="line">  [ <span class="number">0.80849474</span>,  <span class="number">0.87528421</span>,  <span class="number">0.94207368</span>,  <span class="number">1.00886316</span>,  <span class="number">1.07565263</span>],</span><br><span class="line">  [ <span class="number">1.14244211</span>,  <span class="number">1.20923158</span>,  <span class="number">1.27602105</span>,  <span class="number">1.34281053</span>,  <span class="number">1.4096</span>    ]])</span><br><span class="line">expected_velocity = np.asarray([</span><br><span class="line">  [ <span class="number">0.5406</span>,      <span class="number">0.55475789</span>,  <span class="number">0.56891579</span>, <span class="number">0.58307368</span>,  <span class="number">0.59723158</span>],</span><br><span class="line">  [ <span class="number">0.61138947</span>,  <span class="number">0.62554737</span>,  <span class="number">0.63970526</span>,  <span class="number">0.65386316</span>,  <span class="number">0.66802105</span>],</span><br><span class="line">  [ <span class="number">0.68217895</span>,  <span class="number">0.69633684</span>,  <span class="number">0.71049474</span>,  <span class="number">0.72465263</span>,  <span class="number">0.73881053</span>],</span><br><span class="line">  [ <span class="number">0.75296842</span>,  <span class="number">0.76712632</span>,  <span class="number">0.78128421</span>,  <span class="number">0.79544211</span>,  <span class="number">0.8096</span>    ]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Should see relative errors around e-8 or less</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;next_w error: &quot;</span>, rel_error(next_w, expected_next_w))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;velocity error: &quot;</span>, rel_error(expected_velocity, config[<span class="string">&quot;velocity&quot;</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>5-训练网络</p>
<p>训练一个同时具有 SGD 和 SGD+momentum 的六层网络。 您应该会看到 SGD+momentum 更新规则收敛得更快</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">num_train = <span class="number">4000</span>                              <span class="comment"># 输入图像4000张</span></span><br><span class="line">small_data = &#123;                              <span class="comment"># 划分训练集和验证集</span></span><br><span class="line">  <span class="string">&#x27;X_train&#x27;</span>: data[<span class="string">&#x27;X_train&#x27;</span>][:num_train],</span><br><span class="line">  <span class="string">&#x27;y_train&#x27;</span>: data[<span class="string">&#x27;y_train&#x27;</span>][:num_train],</span><br><span class="line">  <span class="string">&#x27;X_val&#x27;</span>: data[<span class="string">&#x27;X_val&#x27;</span>],</span><br><span class="line">  <span class="string">&#x27;y_val&#x27;</span>: data[<span class="string">&#x27;y_val&#x27;</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">solvers = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> update_rule <span class="keyword">in</span> [<span class="string">&#x27;sgd&#x27;</span>, <span class="string">&#x27;sgd_momentum&#x27;</span>]:     <span class="comment"># 轮流执行&#x27;sgd&#x27;, &#x27;sgd_momentum&#x27; 优化</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Running with &#x27;</span>, update_rule)</span><br><span class="line">    model = FullyConnectedNet(              <span class="comment"># 调用FullyConnectedNet全连接层，设置：六层神经网络，隐藏层100x100x100x100x100个units！！！！</span></span><br><span class="line">        [<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>],</span><br><span class="line">        weight_scale=<span class="number">5e-2</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    solver = Solver(</span><br><span class="line">        model,</span><br><span class="line">        small_data,</span><br><span class="line">        num_epochs=<span class="number">5</span>,</span><br><span class="line">        batch_size=<span class="number">100</span>,</span><br><span class="line">        update_rule=update_rule,</span><br><span class="line">        optim_config=&#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">5e-3</span>&#125;,</span><br><span class="line">        verbose=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line">    solvers[update_rule] = solver</span><br><span class="line">    solver.train()</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">3</span>, <span class="number">1</span>, figsize=(<span class="number">15</span>, <span class="number">15</span>))</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Training loss&#x27;</span>)        <span class="comment"># 设置第一幅图：图题</span></span><br><span class="line">axes[<span class="number">0</span>].set_xlabel(<span class="string">&#x27;Iteration&#x27;</span>)            <span class="comment"># 设置第一幅图：横坐标</span></span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Training accuracy&#x27;</span>)       <span class="comment"># 设置第二幅图：图题</span></span><br><span class="line">axes[<span class="number">1</span>].set_xlabel(<span class="string">&#x27;Epoch&#x27;</span>)                  <span class="comment"># 设置第二幅图：横坐标</span></span><br><span class="line">axes[<span class="number">2</span>].set_title(<span class="string">&#x27;Validation accuracy&#x27;</span>)      <span class="comment"># 设置第三幅图：图题</span></span><br><span class="line">axes[<span class="number">2</span>].set_xlabel(<span class="string">&#x27;Epoch&#x27;</span>)                  <span class="comment"># 设置第三幅图：横坐标</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> update_rule, solver <span class="keyword">in</span> solvers.items():   <span class="comment"># 循环计数</span></span><br><span class="line">    axes[<span class="number">0</span>].plot(solver.loss_history, label=<span class="string">f&quot;loss_<span class="subst">&#123;update_rule&#125;</span>&quot;</span>)  <span class="comment"># plot第一幅图</span></span><br><span class="line">    axes[<span class="number">1</span>].plot(solver.train_acc_history, label=<span class="string">f&quot;train_acc_<span class="subst">&#123;update_rule&#125;</span>&quot;</span>) <span class="comment"># plot第二幅图</span></span><br><span class="line">    axes[<span class="number">2</span>].plot(solver.val_acc_history, label=<span class="string">f&quot;val_acc_<span class="subst">&#123;update_rule&#125;</span>&quot;</span>)    <span class="comment"># plot第三幅图</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> axes:     <span class="comment"># 设置图表样式</span></span><br><span class="line">    ax.legend(loc=<span class="string">&quot;best&quot;</span>, ncol=<span class="number">4</span>)</span><br><span class="line">    ax.grid(linestyle=<span class="string">&#x27;--&#x27;</span>, linewidth=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">plt.show()        <span class="comment"># 显示绘图</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>6-附加：调用并对比优化器 ‘adam’, ‘rmsprop’ 并plot学习曲线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cs231n.optim <span class="keyword">import</span> rmsprop</span><br><span class="line"><span class="keyword">from</span> cs231n.optim <span class="keyword">import</span> adam</span><br><span class="line"></span><br><span class="line">learning_rates = &#123;<span class="string">&#x27;rmsprop&#x27;</span>: <span class="number">1e-4</span>, <span class="string">&#x27;adam&#x27;</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line"><span class="keyword">for</span> update_rule <span class="keyword">in</span> [<span class="string">&#x27;adam&#x27;</span>, <span class="string">&#x27;rmsprop&#x27;</span>]:   <span class="comment"># 对比优化器 &#x27;adam&#x27;, &#x27;rmsprop&#x27; 并plot！！！！</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Running with &#x27;</span>, update_rule)</span><br><span class="line">    model = FullyConnectedNet(</span><br><span class="line">        [<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>],</span><br><span class="line">        weight_scale=<span class="number">5e-2</span></span><br><span class="line">    )</span><br><span class="line">    solver = Solver(</span><br><span class="line">        model,</span><br><span class="line">        small_data,</span><br><span class="line">        num_epochs=<span class="number">5</span>,</span><br><span class="line">        batch_size=<span class="number">100</span>,</span><br><span class="line">        update_rule=update_rule,</span><br><span class="line">        optim_config=&#123;<span class="string">&#x27;learning_rate&#x27;</span>: learning_rates[update_rule]&#125;,</span><br><span class="line">        verbose=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    solvers[update_rule] = solver</span><br><span class="line">    solver.train()</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">3</span>, <span class="number">1</span>, figsize=(<span class="number">15</span>, <span class="number">15</span>))</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Training loss&#x27;</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Training accuracy&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>].set_xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">axes[<span class="number">2</span>].set_title(<span class="string">&#x27;Validation accuracy&#x27;</span>)</span><br><span class="line">axes[<span class="number">2</span>].set_xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> update_rule, solver <span class="keyword">in</span> solvers.items():</span><br><span class="line">    axes[<span class="number">0</span>].plot(solver.loss_history, label=<span class="string">f&quot;<span class="subst">&#123;update_rule&#125;</span>&quot;</span>)</span><br><span class="line">    axes[<span class="number">1</span>].plot(solver.train_acc_history, label=<span class="string">f&quot;<span class="subst">&#123;update_rule&#125;</span>&quot;</span>)</span><br><span class="line">    axes[<span class="number">2</span>].plot(solver.val_acc_history, label=<span class="string">f&quot;<span class="subst">&#123;update_rule&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> axes:</span><br><span class="line">    ax.legend(loc=<span class="string">&#x27;best&#x27;</span>, ncol=<span class="number">4</span>)</span><br><span class="line">    ax.grid(linestyle=<span class="string">&#x27;--&#x27;</span>, linewidth=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="（2）隐藏层layer-py"><a href="#（2）隐藏层layer-py" class="headerlink" title="（2）隐藏层layer.py"></a>（2）<a target="_blank" rel="noopener" href="http://隐藏层layer.py/">隐藏层layer.py</a></h3><p>1-实现affine_forward、affine_backward、relu_forward、relu_backward 和 softmax_loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> <span class="built_in">range</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#————————————前向传播、后向传播——————————————</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">affine_forward</span>(<span class="params">x, w, b</span>):</span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    x_temp = x.reshape(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">    out = x_temp.dot(w) + b</span><br><span class="line">    cache = (x, w, b)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">affine_backward</span>(<span class="params">dout, cache</span>):</span><br><span class="line">    x, w, b = cache</span><br><span class="line">    dx, dw, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    x_temp = np.reshape(x, (x.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">    db = np.<span class="built_in">sum</span>(dout, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dw = np.dot(x_temp.T, dout)</span><br><span class="line">    dx = np.dot(dout, w.T)</span><br><span class="line">    dx = np.reshape(dx, x.shape)</span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br><span class="line"></span><br><span class="line"><span class="comment">#————————————激活函数——————————————</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu_forward</span>(<span class="params">x</span>):</span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    out = np.maximum(<span class="number">0</span>, x)</span><br><span class="line">    cache = x</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu_backward</span>(<span class="params">dout, cache</span>):</span><br><span class="line">    dx, x = <span class="literal">None</span>, cache</span><br><span class="line">    dx = dout</span><br><span class="line">    dx[x &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> dx</span><br><span class="line"></span><br><span class="line"><span class="comment">#————————————softmax——————————————</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    loss, dx = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    num = <span class="built_in">len</span>(x)</span><br><span class="line">    x_scores = x[<span class="built_in">range</span>(num), y]</span><br><span class="line">    loss = np.<span class="built_in">sum</span>(- np.log(np.exp(x_scores) / np.<span class="built_in">sum</span>(np.exp(x), axis=<span class="number">1</span>))) / num</span><br><span class="line">    dx = np.zeros_like(x)</span><br><span class="line">    dx = np.exp(x) / np.<span class="built_in">sum</span>(np.exp(x), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dx[<span class="built_in">range</span>(num), y] -= <span class="number">1</span></span><br><span class="line">    dx /= num</span><br><span class="line">    <span class="keyword">return</span> loss, dx</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="（3）全连接层fc-net-py"><a href="#（3）全连接层fc-net-py" class="headerlink" title="（3）全连接层fc_net.py"></a>（3）全连接层fc_net.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> <span class="built_in">range</span></span><br><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> <span class="built_in">object</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ..layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> ..layer_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FullyConnectedNet</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="comment">##————————————————————1-实例初始化——————————————————————————————</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        hidden_dims,</span></span><br><span class="line"><span class="params">        input_dim=<span class="number">3</span> * <span class="number">32</span> * <span class="number">32</span>,</span></span><br><span class="line"><span class="params">        num_classes=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">        <span class="comment"># ！如果网络的构造函数接收到的dropout参数的值不是1，那么网络应该在ReLU非线性后立即添加dropout。</span></span></span><br><span class="line"><span class="params">        dropout_keep_ratio=<span class="number">1</span>,  <span class="comment"># 这里参数名不一样，dropout=1，默认无dropout</span></span></span><br><span class="line"><span class="params">        <span class="comment"># dropout_keep_ratio,</span></span></span><br><span class="line"><span class="params">        normalization=<span class="literal">None</span>,  <span class="comment"># 初始值 normalization=None, 默认无参数缩放</span></span></span><br><span class="line"><span class="params">        reg=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        weight_scale=<span class="number">1e-2</span>,</span></span><br><span class="line"><span class="params">        dtype=np.float32,</span></span><br><span class="line"><span class="params">        seed=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.normalization = normalization   <span class="comment"># 这里初始值规定了 normalization=None,</span></span><br><span class="line">        <span class="variable language_">self</span>.use_dropout = dropout_keep_ratio != <span class="number">1</span> <span class="comment"># 这里写的不一样！！！</span></span><br><span class="line">        <span class="variable language_">self</span>.reg = reg</span><br><span class="line">        <span class="variable language_">self</span>.num_layers = <span class="number">1</span> + <span class="built_in">len</span>(hidden_dims)</span><br><span class="line">        <span class="variable language_">self</span>.dtype = dtype</span><br><span class="line">        <span class="variable language_">self</span>.params = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#初始化所有隐藏层的参数</span></span><br><span class="line">        in_dim = input_dim <span class="comment">#D</span></span><br><span class="line">        <span class="keyword">for</span> i,h_dim <span class="keyword">in</span> <span class="built_in">enumerate</span>(hidden_dims):    <span class="comment"># 隐藏层(0,H1)(1,H2)</span></span><br><span class="line">            <span class="variable language_">self</span>.params[<span class="string">&#x27;W%d&#x27;</span> %(i+<span class="number">1</span>,)] = weight_scale * np.random.randn(in_dim,h_dim)</span><br><span class="line">            <span class="variable language_">self</span>.params[<span class="string">&#x27;b%d&#x27;</span> %(i+<span class="number">1</span>,)] = np.zeros((h_dim,))</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.normalization==<span class="string">&#x27;batchnorm&#x27;</span>:   <span class="comment"># 如果存在 batchnorm</span></span><br><span class="line">                <span class="variable language_">self</span>.params[<span class="string">&#x27;gamma%d&#x27;</span> %(i+<span class="number">1</span>,)] = np.ones((h_dim,)) <span class="comment">#初始化为1，把经过 batch normalization 缩放过的参数存入gamma1/2,beta1/2</span></span><br><span class="line">                <span class="variable language_">self</span>.params[<span class="string">&#x27;beta%d&#x27;</span> %(i+<span class="number">1</span>,)] = np.zeros((h_dim,)) <span class="comment">#初始化为0，把经过 batch normalization 缩放过的参数存入gamma1/2,beta1/2</span></span><br><span class="line">            in_dim = h_dim  <span class="comment">#将该层的列数传递给下一层的行数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#初始化所有输出层的参数</span></span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&#x27;W%d&#x27;</span> %(<span class="variable language_">self</span>.num_layers,)] = weight_scale * np.random.randn(in_dim,num_classes) <span class="comment"># 把每层的参数存储到W和b, W初始化用weight_scale</span></span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&#x27;b%d&#x27;</span> %(<span class="variable language_">self</span>.num_layers,)] = np.zeros((num_classes,))                           <span class="comment"># 把每层的参数存储到W和b, b初始化用0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ————————当开启 dropout 时，我们需要在每一个神经元层中传递一个相同的 dropout 参数字典 self.dropout_param ，以保证每一层的神经元们 都知晓失活概率p和当前神经网络的模式状态mode(训练／测试)。</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout_param = &#123;&#125;     <span class="comment">#dropout的参数字典</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_dropout:</span><br><span class="line">            <span class="variable language_">self</span>.dropout_param = &#123;<span class="string">&quot;mode&quot;</span>: <span class="string">&quot;train&quot;</span>, <span class="string">&quot;p&quot;</span>: dropout_keep_ratio&#125;   <span class="comment">#!!!!!这里dropout_keep_ratio=1，即self.use_dropout=0</span></span><br><span class="line">            <span class="keyword">if</span> seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="variable language_">self</span>.dropout_param[<span class="string">&quot;seed&quot;</span>] = seed</span><br><span class="line"></span><br><span class="line">        <span class="comment">#——————当开启批量归一化时，我们要定义一个BN算法的参数列表 self.bn_params ， 以用来跟踪记录每一层的平均值和标准差。其中，第0个元素 self.bn_params[0] 表示前向传播第1个BN层的参数，第1个元素 self.bn_params[1] 表示前向传播 第2个BN层的参数，以此类推</span></span><br><span class="line">        <span class="variable language_">self</span>.bn_params = []         <span class="comment"># BN的参数字典</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.normalization == <span class="string">&quot;batchnorm&quot;</span>:       <span class="comment"># 这里添加了batchnorm 批量标准化</span></span><br><span class="line">            <span class="variable language_">self</span>.bn_params = [&#123;<span class="string">&quot;mode&quot;</span>: <span class="string">&quot;train&quot;</span>&#125; <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_layers - <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.normalization == <span class="string">&quot;layernorm&quot;</span>:     <span class="comment"># 这里添加了layernorm 层标准化</span></span><br><span class="line">            <span class="variable language_">self</span>.bn_params = [&#123;&#125; <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_layers - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast all parameters to the correct datatype.</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="variable language_">self</span>.params.items(): <span class="comment"># 将所有参数转换为正确的数据类型</span></span><br><span class="line">            <span class="variable language_">self</span>.params[k] = v.astype(dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment">##————————————————————2-定义loss——————————————————————————————</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span><br><span class="line">        X = X.astype(<span class="variable language_">self</span>.dtype)</span><br><span class="line">        mode = <span class="string">&#x27;test&#x27;</span> <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">&#x27;train&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set train/test mode for batchnorm params and dropout param since they</span></span><br><span class="line">        <span class="comment"># behave differently during training and testing.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_dropout:</span><br><span class="line">            <span class="variable language_">self</span>.dropout_param[<span class="string">&#x27;mode&#x27;</span>] = mode</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.normalization == <span class="string">&#x27;batchnorm&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> bn_param <span class="keyword">in</span> <span class="variable language_">self</span>.bn_params:</span><br><span class="line">                bn_param[<span class="string">&#x27;mode&#x27;</span>] = mode</span><br><span class="line">        scores = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># VScode！！！</span></span><br><span class="line">        scores = X</span><br><span class="line">        caches = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_layers):</span><br><span class="line">            w = <span class="variable language_">self</span>.params[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">            b = <span class="variable language_">self</span>.params[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">            <span class="keyword">if</span> i == <span class="variable language_">self</span>.num_layers - <span class="number">1</span>:</span><br><span class="line">                scores, cache = affine_forward(scores, w, b)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.normalization <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    scores, cache = affine_relu_forward(scores, w, b)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    gamma = <span class="variable language_">self</span>.params[<span class="string">&#x27;gamma&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">                    beta = <span class="variable language_">self</span>.params[<span class="string">&#x27;beta&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">                    <span class="keyword">if</span> <span class="variable language_">self</span>.normalization == <span class="string">&#x27;batchnorm&#x27;</span>:</span><br><span class="line">                        scores, cache = affine_bn_relu_forward(scores, w, b, gamma, beta, <span class="variable language_">self</span>.bn_params[i])</span><br><span class="line">                    <span class="keyword">elif</span> <span class="variable language_">self</span>.normalization == <span class="string">&#x27;layernorm&#x27;</span>:</span><br><span class="line">                        scores, cache = affine_ln_relu_forward(scores, w, b, gamma, beta, <span class="variable language_">self</span>.bn_params[i])</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        cache = <span class="literal">None</span></span><br><span class="line">            caches.append(cache)</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_dropout <span class="keyword">and</span> i != <span class="variable language_">self</span>.num_layers - <span class="number">1</span>:</span><br><span class="line">                scores, cache = dropout_forward(scores, <span class="variable language_">self</span>.dropout_param)</span><br><span class="line">                caches.append(cache)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If test mode return early.</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&quot;test&quot;</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line"><span class="comment">##————————————————————3-定义梯度——————————————————————————————</span></span><br><span class="line">        loss, grads = <span class="number">0.0</span>, &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># VScode！！！</span></span><br><span class="line">        loss, dx = softmax_loss(scores, y)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_layers):</span><br><span class="line">            loss += <span class="number">0.5</span> * <span class="variable language_">self</span>.reg * np.<span class="built_in">sum</span>(<span class="variable language_">self</span>.params[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] ** <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="variable language_">self</span>.num_layers)):</span><br><span class="line">            w = <span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)</span><br><span class="line">            b = <span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)</span><br><span class="line">            gamma = <span class="string">&#x27;gamma&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)</span><br><span class="line">            beta = <span class="string">&#x27;beta&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> i == <span class="variable language_">self</span>.num_layers - <span class="number">1</span>:</span><br><span class="line">                dx, grads[w], grads[b] = affine_backward(dx, caches.pop())</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.use_dropout:</span><br><span class="line">                    dx = dropout_backward(dx, caches.pop())</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.normalization <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    dx, grads[w], grads[b] = affine_relu_backward(dx, caches.pop())</span><br><span class="line">                <span class="keyword">elif</span> <span class="variable language_">self</span>.normalization == <span class="string">&#x27;batchnorm&#x27;</span>:</span><br><span class="line">                    dx, grads[w], grads[b], grads[gamma], grads[beta] = affine_bn_relu_backward(dx, caches.pop())</span><br><span class="line">                <span class="keyword">elif</span> <span class="variable language_">self</span>.normalization == <span class="string">&#x27;layernorm&#x27;</span>:</span><br><span class="line">                    dx, grads[w], grads[b], grads[gamma], grads[beta] = affine_ln_relu_backward(dx, caches.pop())</span><br><span class="line"></span><br><span class="line">            grads[w] += <span class="variable language_">self</span>.reg * <span class="variable language_">self</span>.params[w]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="4-optim-py"><a href="#4-optim-py" class="headerlink" title="(4)optim.py"></a>(4)<a target="_blank" rel="noopener" href="http://optim.py/">optim.py</a></h3><p>1- 定义sdg + sgd_momentum</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">w, dw, config=<span class="literal">None</span></span>): <span class="comment"># 定义随机梯度下降算法！！！！</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Performs vanilla stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    config format:</span></span><br><span class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">&quot;learning_rate&quot;</span>, <span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line">    w -= config[<span class="string">&quot;learning_rate&quot;</span>] * dw</span><br><span class="line">    <span class="keyword">return</span> w, config</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd_momentum</span>(<span class="params">w, dw, config=<span class="literal">None</span></span>): <span class="comment"># 定义sgd_momentum算法！！！</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">&quot;learning_rate&quot;</span>, <span class="number">1e-2</span>)</span><br><span class="line">    config.setdefault(<span class="string">&quot;momentum&quot;</span>, <span class="number">0.9</span>)</span><br><span class="line">    v = config.get(<span class="string">&quot;velocity&quot;</span>, np.zeros_like(w))</span><br><span class="line">    next_w = <span class="literal">None</span></span><br><span class="line">    v = config[<span class="string">&#x27;momentum&#x27;</span>] * v - config[<span class="string">&#x27;learning_rate&#x27;</span>] * dw</span><br><span class="line">    next_w = w + v</span><br><span class="line">    config[<span class="string">&quot;velocity&quot;</span>] = v</span><br><span class="line">    <span class="keyword">return</span> next_w, config</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>2-定义rmsprop优化算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rmsprop</span>(<span class="params">w, dw, config=<span class="literal">None</span></span>): <span class="comment"># 定义rmsprop优化算法！！！！！</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">&quot;learning_rate&quot;</span>, <span class="number">1e-2</span>)</span><br><span class="line">    config.setdefault(<span class="string">&quot;decay_rate&quot;</span>, <span class="number">0.99</span>)</span><br><span class="line">    config.setdefault(<span class="string">&quot;epsilon&quot;</span>, <span class="number">1e-8</span>)</span><br><span class="line">    config.setdefault(<span class="string">&quot;cache&quot;</span>, np.zeros_like(w))</span><br><span class="line">    next_w = <span class="literal">None</span></span><br><span class="line">    cache = config[<span class="string">&#x27;cache&#x27;</span>]</span><br><span class="line">    decay_rate = config[<span class="string">&#x27;decay_rate&#x27;</span>]</span><br><span class="line">    learning_rate = config[<span class="string">&#x27;learning_rate&#x27;</span>]</span><br><span class="line">    epsilon = config[<span class="string">&#x27;epsilon&#x27;</span>]</span><br><span class="line">    cache = decay_rate * cache + (<span class="number">1</span> - decay_rate) * (dw**<span class="number">2</span>)</span><br><span class="line">    w += - learning_rate * dw / (np.sqrt(cache) + epsilon)</span><br><span class="line">    config[<span class="string">&#x27;cache&#x27;</span>] = cache</span><br><span class="line">    next_w = w</span><br><span class="line">    <span class="keyword">return</span> next_w, config</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3-定义Adam优化算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">adam</span>(<span class="params">w, dw, config=<span class="literal">None</span></span>):  <span class="comment"># 定义Adam优化算法！！！！</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">&quot;learning_rate&quot;</span>, <span class="number">1e-3</span>)</span><br><span class="line">    config.setdefault(<span class="string">&quot;beta1&quot;</span>, <span class="number">0.9</span>)</span><br><span class="line">    config.setdefault(<span class="string">&quot;beta2&quot;</span>, <span class="number">0.999</span>)</span><br><span class="line">    config.setdefault(<span class="string">&quot;epsilon&quot;</span>, <span class="number">1e-8</span>)</span><br><span class="line">    config.setdefault(<span class="string">&quot;m&quot;</span>, np.zeros_like(w))</span><br><span class="line">    config.setdefault(<span class="string">&quot;v&quot;</span>, np.zeros_like(w))</span><br><span class="line">    config.setdefault(<span class="string">&quot;t&quot;</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    next_w = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    config[<span class="string">&#x27;t&#x27;</span>] += <span class="number">1</span>         <span class="comment">###  Q2写法</span></span><br><span class="line">    beta1 = config[<span class="string">&#x27;beta1&#x27;</span>]</span><br><span class="line">    beta2 = config[<span class="string">&#x27;beta2&#x27;</span>]</span><br><span class="line">    epsilon = config[<span class="string">&#x27;epsilon&#x27;</span>]</span><br><span class="line">    learning_rate = config[<span class="string">&#x27;learning_rate&#x27;</span>]</span><br><span class="line">    config[<span class="string">&#x27;m&#x27;</span>] = beta1 * config[<span class="string">&#x27;m&#x27;</span>] + (<span class="number">1</span>-beta1) * dw</span><br><span class="line">    config[<span class="string">&#x27;v&#x27;</span>] = beta2 * config[<span class="string">&#x27;v&#x27;</span>] + (<span class="number">1</span>-beta2) * dw**<span class="number">2</span></span><br><span class="line">    mb = config[<span class="string">&#x27;m&#x27;</span>]/(<span class="number">1</span> - beta1**config[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">    vb = config[<span class="string">&#x27;v&#x27;</span>]/(<span class="number">1</span> - beta2**config[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">    next_w = w - learning_rate * mb / (np.sqrt(vb)+epsilon)</span><br><span class="line">    <span class="keyword">return</span> next_w, config</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="5-solver-py"><a href="#5-solver-py" class="headerlink" title="(5)solver.py"></a>(5)<a target="_blank" rel="noopener" href="http://solver.py/">solver.py</a></h3><p>1-solver为一个执行器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"><span class="keyword">from</span> future <span class="keyword">import</span> standard_library</span><br><span class="line">standard_library.install_aliases()</span><br><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> <span class="built_in">range</span></span><br><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> <span class="built_in">object</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pickle <span class="keyword">as</span> pickle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> cs231n <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solver</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, data, **kwargs</span>):</span><br><span class="line">        <span class="variable language_">self</span>.model = model</span><br><span class="line">        <span class="variable language_">self</span>.X_train = data[<span class="string">&quot;X_train&quot;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.y_train = data[<span class="string">&quot;y_train&quot;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.X_val = data[<span class="string">&quot;X_val&quot;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.y_val = data[<span class="string">&quot;y_val&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Unpack keyword arguments</span></span><br><span class="line">        <span class="variable language_">self</span>.update_rule = kwargs.pop(<span class="string">&quot;update_rule&quot;</span>, <span class="string">&quot;sgd&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.optim_config = kwargs.pop(<span class="string">&quot;optim_config&quot;</span>, &#123;&#125;)</span><br><span class="line">        <span class="variable language_">self</span>.lr_decay = kwargs.pop(<span class="string">&quot;lr_decay&quot;</span>, <span class="number">1.0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.batch_size = kwargs.pop(<span class="string">&quot;batch_size&quot;</span>, <span class="number">100</span>)</span><br><span class="line">        <span class="variable language_">self</span>.num_epochs = kwargs.pop(<span class="string">&quot;num_epochs&quot;</span>, <span class="number">10</span>)</span><br><span class="line">        <span class="variable language_">self</span>.num_train_samples = kwargs.pop(<span class="string">&quot;num_train_samples&quot;</span>, <span class="number">1000</span>)</span><br><span class="line">        <span class="variable language_">self</span>.num_val_samples = kwargs.pop(<span class="string">&quot;num_val_samples&quot;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.checkpoint_name = kwargs.pop(<span class="string">&quot;checkpoint_name&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="variable language_">self</span>.print_every = kwargs.pop(<span class="string">&quot;print_every&quot;</span>, <span class="number">10</span>)</span><br><span class="line">        <span class="variable language_">self</span>.verbose = kwargs.pop(<span class="string">&quot;verbose&quot;</span>, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Throw an error if there are extra keyword arguments</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(kwargs) &gt; <span class="number">0</span>:</span><br><span class="line">            extra = <span class="string">&quot;, &quot;</span>.join(<span class="string">&#x27;&quot;%s&quot;&#x27;</span> % k <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">list</span>(kwargs.keys()))</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Unrecognized arguments %s&quot;</span> % extra)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Make sure the update rule exists, then replace the string</span></span><br><span class="line">        <span class="comment"># name with the actual function</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(optim, <span class="variable language_">self</span>.update_rule):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Invalid update_rule &quot;%s&quot;&#x27;</span> % <span class="variable language_">self</span>.update_rule)</span><br><span class="line">        <span class="variable language_">self</span>.update_rule = <span class="built_in">getattr</span>(optim, <span class="variable language_">self</span>.update_rule)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>._reset()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_reset</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># Set up some variables for book-keeping</span></span><br><span class="line">        <span class="variable language_">self</span>.epoch = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.best_val_acc = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.best_params = &#123;&#125;</span><br><span class="line">        <span class="variable language_">self</span>.loss_history = []</span><br><span class="line">        <span class="variable language_">self</span>.train_acc_history = []</span><br><span class="line">        <span class="variable language_">self</span>.val_acc_history = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Make a deep copy of the optim_config for each parameter</span></span><br><span class="line">        <span class="variable language_">self</span>.optim_configs = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.model.params:</span><br><span class="line">            d = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="variable language_">self</span>.optim_config.items()&#125;</span><br><span class="line">            <span class="variable language_">self</span>.optim_configs[p] = d</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># Make a minibatch of training data</span></span><br><span class="line">        num_train = <span class="variable language_">self</span>.X_train.shape[<span class="number">0</span>]</span><br><span class="line">        batch_mask = np.random.choice(num_train, <span class="variable language_">self</span>.batch_size)</span><br><span class="line">        X_batch = <span class="variable language_">self</span>.X_train[batch_mask]</span><br><span class="line">        y_batch = <span class="variable language_">self</span>.y_train[batch_mask]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute loss and gradient</span></span><br><span class="line">        loss, grads = <span class="variable language_">self</span>.model.loss(X_batch, y_batch)</span><br><span class="line">        <span class="variable language_">self</span>.loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform a parameter update</span></span><br><span class="line">        <span class="keyword">for</span> p, w <span class="keyword">in</span> <span class="variable language_">self</span>.model.params.items():</span><br><span class="line">            dw = grads[p]</span><br><span class="line">            config = <span class="variable language_">self</span>.optim_configs[p]</span><br><span class="line">            next_w, next_config = <span class="variable language_">self</span>.update_rule(w, dw, config)</span><br><span class="line">            <span class="variable language_">self</span>.model.params[p] = next_w</span><br><span class="line">            <span class="variable language_">self</span>.optim_configs[p] = next_config</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_save_checkpoint</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.checkpoint_name <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        checkpoint = &#123;</span><br><span class="line">            <span class="string">&quot;model&quot;</span>: <span class="variable language_">self</span>.model,</span><br><span class="line">            <span class="string">&quot;update_rule&quot;</span>: <span class="variable language_">self</span>.update_rule,</span><br><span class="line">            <span class="string">&quot;lr_decay&quot;</span>: <span class="variable language_">self</span>.lr_decay,</span><br><span class="line">            <span class="string">&quot;optim_config&quot;</span>: <span class="variable language_">self</span>.optim_config,</span><br><span class="line">            <span class="string">&quot;batch_size&quot;</span>: <span class="variable language_">self</span>.batch_size,</span><br><span class="line">            <span class="string">&quot;num_train_samples&quot;</span>: <span class="variable language_">self</span>.num_train_samples,</span><br><span class="line">            <span class="string">&quot;num_val_samples&quot;</span>: <span class="variable language_">self</span>.num_val_samples,</span><br><span class="line">            <span class="string">&quot;epoch&quot;</span>: <span class="variable language_">self</span>.epoch,</span><br><span class="line">            <span class="string">&quot;loss_history&quot;</span>: <span class="variable language_">self</span>.loss_history,</span><br><span class="line">            <span class="string">&quot;train_acc_history&quot;</span>: <span class="variable language_">self</span>.train_acc_history,</span><br><span class="line">            <span class="string">&quot;val_acc_history&quot;</span>: <span class="variable language_">self</span>.val_acc_history,</span><br><span class="line">        &#125;</span><br><span class="line">        filename = <span class="string">&quot;%s_epoch_%d.pkl&quot;</span> % (<span class="variable language_">self</span>.checkpoint_name, <span class="variable language_">self</span>.epoch)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.verbose:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Saving checkpoint to &quot;%s&quot;&#x27;</span> % filename)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(checkpoint, f)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_accuracy</span>(<span class="params">self, X, y, num_samples=<span class="literal">None</span>, batch_size=<span class="number">100</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Maybe subsample the data</span></span><br><span class="line">        N = X.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> num_samples <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> N &gt; num_samples:</span><br><span class="line">            mask = np.random.choice(N, num_samples)</span><br><span class="line">            N = num_samples</span><br><span class="line">            X = X[mask]</span><br><span class="line">            y = y[mask]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute predictions in batches</span></span><br><span class="line">        num_batches = N // batch_size</span><br><span class="line">        <span class="keyword">if</span> N % batch_size != <span class="number">0</span>:</span><br><span class="line">            num_batches += <span class="number">1</span></span><br><span class="line">        y_pred = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">            start = i * batch_size</span><br><span class="line">            end = (i + <span class="number">1</span>) * batch_size</span><br><span class="line">            scores = <span class="variable language_">self</span>.model.loss(X[start:end])</span><br><span class="line">            y_pred.append(np.argmax(scores, axis=<span class="number">1</span>))</span><br><span class="line">        y_pred = np.hstack(y_pred)</span><br><span class="line">        acc = np.mean(y_pred == y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self</span>):</span><br><span class="line">        num_train = <span class="variable language_">self</span>.X_train.shape[<span class="number">0</span>]</span><br><span class="line">        iterations_per_epoch = <span class="built_in">max</span>(num_train // <span class="variable language_">self</span>.batch_size, <span class="number">1</span>)</span><br><span class="line">        num_iterations = <span class="variable language_">self</span>.num_epochs * iterations_per_epoch</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">            <span class="variable language_">self</span>._step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Maybe print training loss</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.verbose <span class="keyword">and</span> t % <span class="variable language_">self</span>.print_every == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(</span><br><span class="line">                    <span class="string">&quot;(Iteration %d / %d) loss: %f&quot;</span></span><br><span class="line">                    % (t + <span class="number">1</span>, num_iterations, <span class="variable language_">self</span>.loss_history[-<span class="number">1</span>])</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># At the end of every epoch, increment the epoch counter and decay</span></span><br><span class="line">            <span class="comment"># the learning rate.</span></span><br><span class="line">            epoch_end = (t + <span class="number">1</span>) % iterations_per_epoch == <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> epoch_end:</span><br><span class="line">                <span class="variable language_">self</span>.epoch += <span class="number">1</span></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="variable language_">self</span>.optim_configs:</span><br><span class="line">                    <span class="variable language_">self</span>.optim_configs[k][<span class="string">&quot;learning_rate&quot;</span>] *= <span class="variable language_">self</span>.lr_decay</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check train and val accuracy on the first iteration, the last</span></span><br><span class="line">            <span class="comment"># iteration, and at the end of each epoch.</span></span><br><span class="line">            first_it = t == <span class="number">0</span></span><br><span class="line">            last_it = t == num_iterations - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> first_it <span class="keyword">or</span> last_it <span class="keyword">or</span> epoch_end:</span><br><span class="line">                train_acc = <span class="variable language_">self</span>.check_accuracy(</span><br><span class="line">                    <span class="variable language_">self</span>.X_train, <span class="variable language_">self</span>.y_train, num_samples=<span class="variable language_">self</span>.num_train_samples</span><br><span class="line">                )</span><br><span class="line">                val_acc = <span class="variable language_">self</span>.check_accuracy(</span><br><span class="line">                    <span class="variable language_">self</span>.X_val, <span class="variable language_">self</span>.y_val, num_samples=<span class="variable language_">self</span>.num_val_samples</span><br><span class="line">                )</span><br><span class="line">                <span class="variable language_">self</span>.train_acc_history.append(train_acc)</span><br><span class="line">                <span class="variable language_">self</span>.val_acc_history.append(val_acc)</span><br><span class="line">                <span class="variable language_">self</span>._save_checkpoint()</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.verbose:</span><br><span class="line">                    <span class="built_in">print</span>(</span><br><span class="line">                        <span class="string">&quot;(Epoch %d / %d) train acc: %f; val_acc: %f&quot;</span></span><br><span class="line">                        % (<span class="variable language_">self</span>.epoch, <span class="variable language_">self</span>.num_epochs, train_acc, val_acc)</span><br><span class="line">                    )</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Keep track of the best model</span></span><br><span class="line">                <span class="keyword">if</span> val_acc &gt; <span class="variable language_">self</span>.best_val_acc:</span><br><span class="line">                    <span class="variable language_">self</span>.best_val_acc = val_acc</span><br><span class="line">                    <span class="variable language_">self</span>.best_params = &#123;&#125;</span><br><span class="line">                    <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="variable language_">self</span>.model.params.items():</span><br><span class="line">                        <span class="variable language_">self</span>.best_params[k] = v.copy()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># At the end of training swap the best params into the model</span></span><br><span class="line">        <span class="variable language_">self</span>.model.params = <span class="variable language_">self</span>.best_params</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="http://2-初始化脚本setup.py/">2-初始化脚本setup.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> distutils.extension <span class="keyword">import</span> Extension</span><br><span class="line"><span class="keyword">from</span> Cython.Build <span class="keyword">import</span> cythonize</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line">extensions = [</span><br><span class="line">    Extension(</span><br><span class="line">        <span class="string">&quot;im2col_cython&quot;</span>, [<span class="string">&quot;im2col_cython.pyx&quot;</span>], include_dirs=[numpy.get_include()]</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line">setup(ext_modules=cythonize(extensions),)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Dr.yuan</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://example.com/2025/06/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DL-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%89%8B%E5%8A%A8%E8%B0%83%E5%8F%82Python/">http://example.com/2025/06/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DL-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%89%8B%E5%8A%A8%E8%B0%83%E5%8F%82Python/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DL/"># 深度学习DL</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2025/06/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DL-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0-Transfer-Learning/">[深度学习DL]迁移学习 Transfer Learning</a>
            
            
            <a class="next" rel="next" href="/2025/06/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DL-%E4%BB%80%E4%B9%88%E6%98%AFTensor%E5%BC%A0%E9%87%8F/">[深度学习DL]什么是Tensor张量</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Dr.yuan | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>